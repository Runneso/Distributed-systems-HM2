# Домашнее задание №2: Single‑Leader репликация

**Дедлайн:** 1 неделя с момента выдачи

**Максимальный балл:** 10 (8 — функциональность, 2 — качество кода)  

**Язык:** Java/Kotlin или Go

**Формат:** Pull Request в репозиторий + защита

---

## Цель работы

Сделать учебный прототип распределённого **in‑memory key‑value хранилища** (KV-store) с **репликацией от одного лидера** (single leader replication).
Также необходимо реализовать CLI для управления состоянием кластера и демонстрации работы репликации.
В результате работы должны быть:

1) Single‑Leader: записи только через лидера  
2) **Sync vs Semi‑Sync vs Async** (когда отдаём `OK`)  
3) **Replication Factor (RF)** как параметр надёжности/latency  
4) **CLI для управления кластером** (membership, лидер, режимы, задержки)  
5) **Бенчмарки + графики** и объяснение результатов

---

## Глоссарий и ограничения 

- **RF** (replication factor) задаёт **целевой уровень подтверждённой записи**: сколько узлов (включая лидера) должно применить запись, чтобы в режиме `sync` считать её успешной.
- **Semi‑sync** добавляет параметр **K** — сколько follower’ов должны подтвердить запись **для быстрого ответа**, при этом лидер продолжает догонять оставшиеся реплики в фоне.
- Ограничение: `1 ≤ K ≤ RF-1`.
    - если `K = RF-1` → это фактически `sync`
    - если `K = 0` → это фактически `async` (в ДЗ считаем K≥1 для semi-sync)
- Важный смысл: **RF важнее как “целевой уровень надёжности” (количество узлов, на которые необходимо доставить данные)
- K (при semi-sync replication) — “количество узлов, после репликации на которых можно возвращать acks клиенту”**.

---

## 1. Архитектура

В системе должно быть минимум **2 типа процессов**:

1) **Node** — узел хранилища (запускается несколько экземпляров: A, B, C, …)  
2) **CLI / Cluster Controller** — консольное приложение, которое:
   - управляет составом кластера (add/remove)
   - назначает лидера (мануально по команде, НЕ нужно реализовывать алгоритм leader election)
   - меняет параметры репликации (async/semi-sync/sync, RF, K, delay)
   - выполняет пользовательские команды (put/get/…)
   - запускает бенчмарки (должно быть отдельной командой)

Допускается, что CLI и “cluster manager” — один процесс для простоты.
Главное: **узлы должны получать обновления membership/конфига по сети** (необходимо использовать TCP-сокеты) и перестраивать список peers.

**Что НЕ требуется в рамках этого ДЗ:**
- RAFT / выбор лидера / failover лидера
- durable storage (файлы/БД) — всё только **in‑memory**
- multi-leader, конфликты, топологии

---

## 2. Модель данных

Хранилище на каждом узле: `Map<String, String>` (или аналог).

Операции:
- `PUT key value` — записать (создать/перезаписать)
- `GET key` — прочитать
- `DUMP` — отладочная команда: вывести локальное состояние узла (для демонстрации)

`DELETE` — **не обязателен** (можно бонусом).

Ограничения:
- `key` — непустая строка без пробелов
- `value` — строка (может содержать пробелы, если ваш протокол это поддерживает)

---

## 3. Сетевой протокол

### 3.1. Транспорт
- TCP-сокеты
- Node должен принимать:
  1) клиентские запросы (от CLI)
  2) репликационные сообщения (от лидера)
  3) membership/config updates (от CLI/Controller)

Допустимо принимать всё на одном порту и различать по полю `type`.

### 3.2. Формат сообщений
- **JSON Lines** (рекомендуется): 1 JSON на строку, завершается `\n`

**Обязательные поля у любого запроса от CLI:**
- `requestId` (UUID или уникальная строка)
- `clientId` (строка; нужна для бенчмарка/логов)
- `type`: `CLIENT_PUT | CLIENT_GET | CLIENT_DUMP | ...`

**Обязательные поля у репликационного события:**
- `operationId` (UUID)
- `originNodeId` (в single-leader это будет лидер)
- `operationType`: `PUT` (и `DELETE`, если сделали бонусом)
- `key`
- `value` (для PUT)

**Ответы и ошибки**
В ответе должны быть:
- `requestId`
- `status`: `OK | ERROR`
- если `ERROR`: `errorCode` из списка (минимум):
  - `NOT_LEADER`
  - `NOT_ENOUGH_REPLICAS`
  - `TIMEOUT`
  - `BAD_REQUEST`
  - `UNKNOWN_NODE`

> Для удобства проверки: в `NOT_LEADER` желательно возвращать `leaderNodeId`/адрес лидера.

### 3.3. Идемпотентность / защита от дубликатов
Любая репликационная операция должна применяться **не более одного раза**.

Требования:
- на follower хранить in‑memory `seenOpIds` с TTL (например, 5 минут)
- если `opId` уже встречался — операция игнорируется (но можно отвечать `ACK` повторно)
- реализовать периодическую очистку устаревших `opId`

---

## 4. CLI: команды управления и пользовательские команды

CLI может быть интерактивным REPL **и/или** “одна команда за запуск”, но должно быть удобно запускать **скриптом** (для проверки и бенчмарков).

### 4.1. Управление кластером через CLI
- `addNode <nodeId> <host> <port>`
- `removeNode <nodeId>`
- `listNodes`
- `setLeader <nodeId>`

- `setReplication async|semi-sync|sync`
- `setRF <int>` (1..N)
- `setSemiSyncAcks <int>` (K; актуально только в режиме `semi-sync`)
- `setReplicationDelayMs <min> <max>`

**Семантика:**
- после `addNode/removeNode/setLeader/setReplication/setRF/setSemiSyncAcks/setReplicationDelayMs` CLI должен разослать всем живым узлам `CLUSTER_UPDATE` (membership + конфиг).
- узлы должны обновить peers/параметры и не падать.

**Правила валидации конфига (обязательные):**
- `RF <= clusterSize` иначе ошибка
- при `replication=semi-sync`: `1 <= K <= RF-1` иначе ошибка
- при `replication=sync`: `K` игнорируется
- при `replication=async`: `K` игнорируется

### 4.2. Пользовательские команды CLI
- `put <key> <value> [--target <nodeId>] [--client <clientId>]`
- `get <key> [--target <nodeId>] [--client <clientId>]`
- `dump [--target <nodeId>]`

`--target` нужен для демонстрации: отправлять запрос специально на follower/leader.

---

## 5. Single‑Leader репликация

### 5.1. Роли и правила
- В кластере ровно **1 leader**.
- **PUT** принимается **только на leader**.
- Если PUT приходит на follower, follower должен:
  - либо вернуть `ERROR NOT_LEADER` (+ подсказать лидера),
  - либо проксировать запрос на лидера.

- **GET** разрешён на любом узле (leader или follower).  
  В async/semi-sync режимах чтение с follower может быть **устаревшим** — это нормально, это часть задания.

### 5.2. Replication Factor (RF)
В этом ДЗ **RF = число узлов, которые должны подтвердить запись**, включая лидера.

Пример для кластера из 3 узлов:
- RF=1: достаточно применить на leader
- RF=2: leader + 1 follower
- RF=3: leader + 2 followers

Если RF больше числа узлов в кластере — `setRF` должен возвращать ошибку.

### 5.3. Общая механика репликации (для всех режимов)
При `PUT` на лидере:
1) лидер применяет запись локально (в своём Map)
2) создаёт `operationId`
3) отправляет `REPL_PUT(operationId, key, value)` всем followers (или всем доступным на момент записи)
4) follower при применении отправляет `ACK(operationId)` лидеру

**Требование к устойчивости коммуникации:**
- при временной недоступности follower лидер должен иметь **очередь pending operations** в памяти и делать ретраи (конфиги ретраев можно захардкодить).
- гарантия доставки после перезапуска не нужна.

### 5.4. Async replication
В режиме `async` лидер:
- отвечает клиенту `OK` **сразу** после локального применения
- репликация и ретраи идут в фоне

### 5.5. Sync replication
В режиме `sync` лидер:
- отвечает `OK` **только когда** получил `ACK` минимум от **(RF-1)** followers до таймаута
- если не набрал — отвечает `ERROR NOT_ENOUGH_REPLICAS` (или `TIMEOUT`, но смысл “не набрали RF”)

### 5.6. Semi‑Sync replication
В режиме `semi-sync` лидер использует **два порога**:

- **K (semiSyncAcks)** — сколько follower’ов должны подтвердить запись, чтобы **быстро** вернуть `OK`
- **RF-1** — сколько подтверждений нужно, чтобы считать запись “дотянутой до целевого RF” (в фоне)

Поведение строгое:

1) лидер применяет запись локально и рассылает репликацию followers
2) лидер **ждёт ACK от K followers** до таймаута `hotPathTimeoutMs`
   - если получил K → возвращает клиенту `OK`
   - если не получил K → `ERROR NOT_ENOUGH_REPLICAS`
3) независимо от ответа клиенту, лидер **продолжает** ретраить доставку и стремится дотянуть до **(RF-1)** подтверждений в фоне (best effort в рамках жизни процесса)

Ограничения:
- `1 <= K <= RF-1`
- если `K = RF-1` — это эквивалент `sync` (так и должно быть)

**Что именно гарантирует semi-sync (на уровне этого ДЗ):**
- если лидер успел получить ACK от K followers, то запись точно применена минимум на `1 (leader) + K` узлах
- однако запись могла ещё не быть применена на `RF` узлах в момент ответа клиенту → возможны stale reads на отстающих followers (как в async, но обычно реже)

### 5.7. Инъекция задержек репликации (обязательная для демонстрации)
Настройка `setReplicationDelayMs <min> <max>` должна добавлять случайную задержку (random) к репликации **на follower**
(перед применением) или **на отправке лидером** — на ваш выбор.

---

## 6. Обязательные демонстрационные сценарии

В `demo-scenario.md` должны быть пошаговые команды, которые воспроизводят:

### Сценарий 1 — follower отклоняет запись
- отправить `put` на follower через `--target`
- получить `NOT_LEADER` (или показать проксирование, если вы выбрали proxy)

### Сценарий 2 — async lag + stale read
- включить `async` и задержку репликации
- сделать `put` на leader
- сразу сделать `get` с follower → иногда вернёт старое/пустое значение
- через некоторое время повторить `get` → значение появится

### Сценарий 3 — sync + RF
Для кластера из 3 узлов:
- `sync`, RF=3: запись проходит при наличии 2 followers
- остановить 1 follower → запись всё ещё проходит при RF=2, но падает при RF=3
- остановить 2 followers → запись падает при RF>=2 с `NOT_ENOUGH_REPLICAS`

### Сценарий 4 — semi‑sync: быстрое OK, но догоняем в фоне
Для кластера из 3 узлов:
- `semi-sync`, RF=3, K=1, включить задержку репликации
- сделать `put` на leader
- показать, что `OK` приходит быстро (после 1 ACK), но один follower может ещё отставать (stale read)
- дождаться догонки и показать, что значение появилось на всех

---

## 7. Бенчмарки и отчёт (обязательная часть)

### 7.1. Что измеряем
Бенчмарк должен собирать:
- throughput (ops/sec)
- latency: average, p50, p75, p95, p99 (мс)
- бенчмарки отдельно для PUT и GET

Результаты сохранять в файл:
- `benchmarks/results.csv` (или JSON) в репозитории.

Рекомендуемые колонки CSV:
```text
replicationMode,rf,k,threads,putRatio,totalOps,throughputOpsSec,avgMs,p50Ms,p75Ms,p95Ms,p99Ms
```

### 7.2. Обязательные сценарии нагрузочного тестирования
Кластер: **3 узла**. Задержка репликации для бенчмарков должна быть `0`.

#### Часть A: сравнение sync vs async по RF — 12 прогонов
Нужно выполнить минимум **12 прогонов**:
- replication: `sync` и `async` (2 режима)
- RF: `1, 2, 3` (3 значения)
- workload profile:
  - write-heavy: `putRatio = 0.8`
  - read-heavy: `putRatio = 0.2` (80% GET)
- threads: фиксированно `16` (или другое одно число для всех прогонов)

Итого: 2 * 3 * 2 = 12.

#### Часть B: сравнение semi-sync с sync/async — 6 прогонов
Фиксируем: `RF=3`, `threads=16`.

Сделайте:
- replication: `async`, `semi-sync(K=1)`, `sync` (3 режима)
- workload: `putRatio=0.8` и `putRatio=0.2` (2 профиля)

Итого: 3 * 2 = 6.

**Всего минимум: 18 прогонов.**

### 7.3. Требования к report.md
`report.md` должен содержать:
- таблицу всех прогонов (конфигурация → throughput, p50, p95)
- минимум 3 графика:
  1) throughput vs RF (2 линии: sync, async) — отдельно для write-heavy и read-heavy (можно 2 графика)
  2) p95 latency vs RF (аналогично)
  3) сравнение `async vs semi-sync vs sync` при RF=3 (throughput и/или p95) — 1 график
- объяснение наблюдений:
  - почему sync становится медленнее при росте RF
  - почему async быстрее, но даёт stale reads
  - что даёт semi-sync: почему он между sync и async (и в чём риски)
  - почему read-heavy профиль ведёт себя иначе, чем write-heavy (в single-leader)

---

## 8. Формат сдачи

В репозитории должно быть:

1) `README.md`
   - как собрать и запустить Node/CLI
   - краткая архитектура
   - спецификация протокола
   - как запускать бенчмарки
   - какая семантика follower write: redirect или proxy

2) `demo-scenario.md`
   - пошаговые команды для сценариев из раздела 6

3) `report.md` + `benchmarks/` (CSV + графики)

Сдача: PR в репозиторий + защита.

---

## 9. Оценивание (10 баллов)

### 9.1. Функциональность — 8 баллов

#### Задача A — База: KV + протокол + CLI (1.5 балла)
- (0.5) Node: `PUT/GET/DUMP` локально
- (0.5) CLI: умеет `put/get/dump` на указанный `--target`
- (0.5) requestId/clientId проходят через запросы, корректные ошибки на BAD_REQUEST

#### Задача B — Membership + лидер (1.5 балла)
- (0.5) `addNode/removeNode/listNodes` работают, узлы получают `CLUSTER_UPDATE`
- (0.5) `setLeader` корректно применяется на узлах
- (0.5) follower корректно обрабатывает запись (NOT_LEADER или proxy — согласно README)

#### Задача C — Async replication + lag (2.0 балла)
- (1.0) async репликация: лидер отвечает сразу, данные доезжают до followers
- (0.5) есть ретраи/очередь: временная недоступность follower не “теряет” операции в рамках жизни лидера
- (0.5) `setReplicationDelayMs` работает и помогает воспроизвести stale read

#### Задача D — Sync + Semi‑Sync + RF + edge cases (2.0 балла)
- (0.8) sync ждёт ACK от (RF-1) followers, корректно собирает RF
- (0.6) semi-sync ждёт ACK от K followers (валидирует 1<=K<=RF-1) и продолжает догонять в фоне
- (0.3) таймауты работают, нет вечных зависаний
- (0.3) корректная ошибка `NOT_ENOUGH_REPLICAS` при нехватке подтверждений

#### Задача E — Benchmarks + отчёт (1.0 балл)
- (0.5) бенчмарк сохраняет результаты (CSV/JSON)
- (0.5) `report.md`: графики + объяснения trade-offs (включая semi-sync)

### 9.2. Качество кода — 2 балла
- (1.0) архитектура: разумное разделение (network/replication/storage/cli), отсутствие spaghetti-code, понятные модели сообщений
- (1.0) надёжность: потокобезопасность, таймауты, аккуратные логи, отсутствие очевидных гонок/deadlock’ов

---

## 10. Бонусные баллы (сумма <= 10)

Бонус может компенсировать недобор по функциональности, но итоговая оценка ≤ 10.

- (+1.0) Сдача + защита ДЗ первым из всей группы
- (+0.5) `DELETE` с репликацией (tombstone, чтобы удаление не “воскресало”)
- (+0.5) “Read-your-writes” режим в CLI: `get --read leader` (чтение с лидера как решение stale reads)

---

## Checklist перед сдачей

Необходимо проверить свое решение перед защитой, чтобы не тратить время защиты впустую.

Checklist:

- [ ] Follower не принимает запись (NOT_LEADER/proxy) согласно README
- [ ] Sync: RF работает, есть NOT_ENOUGH_REPLICAS, есть таймаут
- [ ] Semi-sync: есть K, валидируется (1<=K<=RF-1), OK после K ACK, догоняем до RF в фоне
- [ ] Async: лидер отвечает сразу, репликация доезжает позже (видно lag через delay)
- [ ] Есть `opId` + dedup по TTL на follower
- [ ] Бенчмарк выдаёт CSV и построены графики в `report.md`

### Сдача ДЗ преподавателю

1. Запись в Google Calendar на проверку ДЗ [по ссылке](https://calendar.app.google/6akyBDUKQpN6sPek6). Записываться на слот необходимо не позднее, чем за 12 часов до слота.
2. Краткий созвон с преподавателем:
    - Демонстрация функциональности работы приложения
    - Защита работы и ответы на вопросы по работе
    - Ответы на дополнительные вопросы
    - Выставление оценки за домашнюю работу по 10-бальной шкале
3. Дедлайн сдачи преподавателю: 13.02.2026, 8:00 pm


